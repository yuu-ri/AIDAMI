```
## **è®¾è®¡æ–‡æ¡£ï¼šAIé©±åŠ¨çš„â€œAIå‰æ²¿è¿½è¸ªâ€è‡ªåŠ¨åŒ–å·¥ä½œæµç³»ç»Ÿ**

### 1. æ¦‚è¿°ä¸æ„¿æ™¯

**ç›®æ ‡**: æ„å»ºä¸€ä¸ªå…¨è‡ªåŠ¨ã€è‡ªæˆ‘è¿›åŒ–çš„ç³»ç»Ÿï¼Œåˆ©ç”¨ä½ ç°æœ‰çš„AIå·¥ä½œæµå¹³å°ï¼ŒæŒç»­è¿½è¸ªã€åˆ†æã€æ€»ç»“å…¨çƒAIå‰æ²¿åŠ¨æ€ï¼Œå¹¶ç”Ÿæˆé«˜ä»·å€¼ã€ä¸ªæ€§åŒ–çš„æƒ…æŠ¥å’Œå¯æ‰§è¡Œä»»åŠ¡ã€‚

**æœ€ç»ˆäº§ç‰©**: ç³»ç»Ÿæ¯æ—¥è‡ªåŠ¨ç”Ÿæˆä¸€ä»½ã€ŠAIå‰æ²¿æ—¥æŠ¥ã€‹ï¼Œæ¯å‘¨ç”Ÿæˆä¸€ä»½ã€ŠAIè¶‹åŠ¿åˆ†æä¸å­¦ä¹ å‘¨æŠ¥ã€‹ï¼Œå¹¶èƒ½æŒ‰éœ€å¯¹ä»»ä½•æŠ€æœ¯ä¸»é¢˜è¿›è¡Œæ·±åº¦ç ”ç©¶ã€‚è¿™å¥—ç³»ç»Ÿå°†æˆä¸ºä½ çš„â€œAIç ”ç©¶åŠ©ç†â€ï¼Œå°†ä½ ä»ç¹æ‚çš„ä¿¡æ¯æ”¶é›†ä¸­è§£æ”¾å‡ºæ¥ã€‚

**æ ¸å¿ƒç†å¿µ**: **ç”¨AIè¿½è¸ªAI (Using AI to track AI)**ã€‚è¿™ä¸ä»…æ˜¯ä¸€ä¸ªåº”ç”¨ï¼Œæ›´æ˜¯å¯¹ä½ å¹³å°èƒ½åŠ›çš„ä¸€æ¬¡ç»ˆææ£€éªŒå’Œé©±åŠ¨ã€‚

### 2. æ ¸å¿ƒè®¾è®¡åŸåˆ™

1.  **åˆ†å±‚å·¥ä½œæµæ¶æ„ (Layered Architecture)**ï¼šå°†å¤æ‚çš„æŒç»­è¿½è¸ªä»»åŠ¡åˆ†è§£ä¸ºä¸‰ä¸ªä¸åŒé¢‘æ¬¡ã€ä¸åŒæ·±åº¦çš„ç‹¬ç«‹å·¥ä½œæµï¼Œä½¿ç³»ç»Ÿç»“æ„æ¸…æ™°ã€æ˜“äºç®¡ç†å’Œæ‰©å±•ã€‚
    *   **L1 - æ¯æ—¥ä¿¡æ¯é‡‡é›† (Daily Collector)**: é«˜é¢‘ã€è‡ªåŠ¨åŒ–ã€å®½æ³›åœ°æ”¶é›†åŸå§‹æ•°æ®ã€‚**ç›®æ ‡æ˜¯â€œä¸é”™è¿‡â€**ã€‚
    *   **L2 - æ¯å‘¨è¶‹åŠ¿åˆ†æ (Weekly Analyzer)**: ä½é¢‘ã€æ·±åº¦AIåˆ†æï¼Œä»ä¸€å‘¨çš„æ•°æ®ä¸­æç‚¼è¶‹åŠ¿ã€æ´å¯Ÿå’Œå­¦ä¹ è®¡åˆ’ã€‚**ç›®æ ‡æ˜¯â€œçœ‹æ‡‚â€**ã€‚
    *   **L3 - æŒ‰éœ€æ·±åº¦ç ”ç©¶ (On-Demand Deep Dive)**: æ‰‹åŠ¨è§¦å‘ï¼Œé’ˆå¯¹ç‰¹å®šä¸»é¢˜è¿›è¡Œæ·±å…¥ç ”ç©¶ã€èµ„æ–™æ•´ç†å’Œä»£ç ç”Ÿæˆã€‚**ç›®æ ‡æ˜¯â€œå­¦ä¼šâ€**ã€‚

2.  **åŸå­åŒ–ä¸å¯å¤ç”¨çš„å·¥å…·é›† (Atomic & Reusable Tools)**ï¼šä¸ä¾èµ–äºåºå¤§å¤æ‚çš„`curl`å‘½ä»¤ï¼Œè€Œæ˜¯ä¸ºæ¯ä¸ªä¿¡æ¯æºåœ¨`sandbox`ä¸­åˆ›å»ºä¸“ç”¨çš„ã€å¥å£®çš„Shellè„šæœ¬å·¥å…·ã€‚è¿™ä½¿å¾—Planæ›´ç®€æ´ã€æ›´ç¨³å®šã€æ›´æ˜“ç»´æŠ¤ã€‚

3.  **Planå³ä»£ç  (Plan as Code)**ï¼šå°†ä¸‰ä¸ªæ ¸å¿ƒå·¥ä½œæµçš„é€»è¾‘å›ºåŒ–ä¸ºä¸‰ä¸ªç‹¬ç«‹çš„`Plan JSON`æ¨¡æ¿ã€‚é€šè¿‡å¹³å°çš„`plan_data_override`åŠŸèƒ½æ¥è°ƒç”¨è¿™äº›é¢„è®¾å¥½çš„â€œé‡‘ç‰Œå·¥ä½œæµâ€ï¼Œå®ç°é«˜æ•ˆå¤ç”¨ã€‚

4.  **æ•°æ®é©±åŠ¨çš„é—­ç¯ (Data-Driven Loop)**ï¼šL1å·¥ä½œæµçš„äº§å‡ºæ˜¯L2å·¥ä½œæµçš„è¾“å…¥ã€‚L2çš„äº§å‡ºï¼ˆå­¦ä¹ è®¡åˆ’ï¼‰å¯ä»¥è§¦å‘L3å·¥ä½œæµã€‚è¿™å½¢æˆäº†ä¸€ä¸ªä»æ•°æ®é‡‡é›†åˆ°æ·±åº¦å­¦ä¹ çš„å®Œæ•´é—­ç¯ã€‚

### 3. ç³»ç»Ÿæ¶æ„ä¸å®ç°

#### **Phase 1: åŸºç¡€è®¾æ–½ä¸å·¥å…·é›†**

æ­¤é˜¶æ®µçš„ç›®æ ‡æ˜¯ä¸º`sandbox`é…å¤‡ä¸€å¥—å¼ºå¤§çš„ã€ä¸“ç”¨çš„ä¿¡æ¯é‡‡é›†å·¥å…·ã€‚è¿™äº›å·¥å…·æ˜¯æ‰€æœ‰ä¸Šå±‚å·¥ä½œæµçš„åŸºçŸ³ã€‚

**å»ºè®®çš„å·¥å…·åˆ—è¡¨ (Shellè„šæœ¬, ä½äº`sandbox`çš„å·¥å…·ç›®å½•ä¸­):**

1.  **`fetch_arxiv_daily.sh`**:
    *   **åŠŸèƒ½**: è·å–Arxiv `cs.AI`, `cs.LG`, `cs.CL`åˆ†ç±»ä¸‹è¿‡å»24å°æ—¶å†…å‘å¸ƒçš„æœ€æ–°è®ºæ–‡ã€‚
    *   **è¾“å‡º**: è¿”å›ä¸€ä¸ªJSONæ•°ç»„ï¼ŒåŒ…å«`title`, `summary`, `pdf_url`, `published_date`ç­‰å­—æ®µã€‚è„šæœ¬å†…éƒ¨å¤„ç†XMLè§£æå’Œæ—¥æœŸè¿‡æ»¤ã€‚

2.  **`fetch_github_trending_ai.sh`**:
    *   **åŠŸèƒ½**: è·å–GitHub Trendingæ—¥æ¦œï¼Œå¹¶ç­›é€‰å‡ºä»“åº“åæˆ–æè¿°ä¸­åŒ…å«AIç›¸å…³å…³é”®è¯ï¼ˆå¦‚ai, llm, agent, diffusionï¼‰çš„é¡¹ç›®ã€‚
    *   **è¾“å‡º**: è¿”å›ä¸€ä¸ªJSONæ•°ç»„ï¼ŒåŒ…å«`repo_name`, `description`, `stars_today`, `language`ç­‰å­—æ®µã€‚

3.  **`fetch_huggingface_trending.sh`**:
    *   **åŠŸèƒ½**: è°ƒç”¨Hugging Face APIï¼Œè·å–è¶‹åŠ¿æ¨¡å‹ï¼ˆTrending Modelsï¼‰åˆ—è¡¨ã€‚
    *   **è¾“å‡º**: è¿”å›ä¸€ä¸ªJSONæ•°ç»„ï¼ŒåŒ…å«`model_id`, `author`, `downloads`, `tags`ç­‰å­—æ®µã€‚

4.  **`fetch_hackernews_ai.sh`**:
    *   **åŠŸèƒ½**: è°ƒç”¨Hacker News APIæˆ–é€šè¿‡çˆ¬è™«ï¼Œè·å–é¦–é¡µåŒ…å«AI/MLç­‰å…³é”®è¯çš„çƒ­é—¨å¸–å­ã€‚
    *   **è¾“å‡º**: è¿”å›ä¸€ä¸ªJSONæ•°ç»„ï¼ŒåŒ…å«`title`, `url`, `points`, `comments_count`ã€‚

*(å¯æ ¹æ®éœ€è¦æ·»åŠ æ›´å¤šå·¥å…·ï¼Œå¦‚`fetch_tech_blog.sh`ç­‰)*

#### **Phase 2: æ ¸å¿ƒå·¥ä½œæµPlanæ¨¡æ¿**

ä»¥ä¸‹æ˜¯ä¸‰ä¸ªæ ¸å¿ƒå·¥ä½œæµçš„`Plan JSON`æ¨¡æ¿ï¼Œå¯ä¿å­˜ä¸ºæ–‡ä»¶ï¼Œé€šè¿‡`plan_data_override`è°ƒç”¨ã€‚

---

**1. L1 - æ¯æ—¥ä¿¡æ¯é‡‡é›†å·¥ä½œæµ (`daily_collector_plan.json`)**

*   **ç›®æ ‡**: å¹¶è¡ŒæŠ“å–æ‰€æœ‰ä¿¡æºçš„æœ€æ–°æ•°æ®ï¼Œèšåˆåå­˜ä¸ºå¸¦æ—¥æœŸçš„åŸå§‹JSONæ–‡ä»¶ã€‚
*   **è§¦å‘æ–¹å¼**: å®šæ—¶ä»»åŠ¡ (Cron Job / Temporal Schedule)ï¼Œæ¯æ—¥å‡Œæ™¨æ‰§è¡Œã€‚

```json
{
  "steps": [
    {
      "type": "action",
      "action": "execute_command",
      "inputs": {
        "command": "chmod +x sandbox_service/app/tools/*.sh",
        "environment_id": "{{environment_id}}"
      },
      "step_id": "make_tools_executable",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "execute_command",
      "inputs": {
        "command": "sandbox_service/app/tools/arxiv_daily.sh",
        "environment_id": "{{environment_id}}"
      },
      "step_id": "collect_arxiv_papers",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "execute_command",
      "inputs": {
        "command": "sandbox_service/app/tools/fetch_github_trending_ai.sh",
        "environment_id": "{{environment_id}}"
      },
      "step_id": "collect_github_trending",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "execute_command",
      "inputs": {
        "command": "sandbox_service/app/tools/fetch_huggingface_trending.sh",
        "environment_id": "{{environment_id}}"
      },
      "step_id": "collect_huggingface_models",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "execute_command",
      "inputs": {
        "command": "sandbox_service/app/tools/fetch_hackernews_ai.sh",
        "environment_id": "{{environment_id}}"
      },
      "step_id": "collect_hackernews_stories",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "execute_command",
      "inputs": {
        "command": "date +%Y-%m-%d | tr -d '\\n'",
        "environment_id": "{{environment_id}}"
      },
      "step_id": "get_current_date_for_filename",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "execute_command",
      "inputs": {
        "command": "cat <<'EOF' | python3 -m json.tool\n{\n  \"arxiv\": {{collect_arxiv_papers.output.stdout}},\n  \"github\": {{collect_github_trending.output.stdout}},\n  \"huggingface\": {{collect_huggingface_models.output.stdout}},\n  \"hackernews\": {{collect_hackernews_stories.output.stdout}}\n}\nEOF",
        "environment_id": "{{environment_id}}"
      },
      "step_id": "format_json_content",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "write_files",
      "inputs": {
        "files": [
          {
            "content": "{{format_json_content.output.stdout}}",
            "filepath": "data/daily/{{get_current_date_for_filename.output.stdout}}.json"
          }
        ],
        "environment_id": "{{environment_id}}"
      },
      "step_id": "save_aggregated_data_to_file",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "execute_command",
      "inputs": {
        "command": "sudo cp -r data /host/",
        "environment_id": "{{environment_id}}"
      },
      "step_id": "copy_to_host",
      "return_as_final_answer": true
    }
  ]
}
```

---

**2. L2 - æ¯å‘¨è¶‹åŠ¿åˆ†æå·¥ä½œæµ (`weekly_analyzer_plan.json`)**

*   **ç›®æ ‡**: è¯»å–è¿‡å»7å¤©çš„åŸå§‹æ•°æ®ï¼Œè¿›è¡Œæ·±åº¦AIåˆ†æï¼Œç”Ÿæˆå‘¨æŠ¥å’Œå­¦ä¹ è®¡åˆ’ã€‚
*   **è§¦å‘æ–¹å¼**: å®šæ—¶ä»»åŠ¡ï¼Œæ¯å‘¨ä¸€æ—©ä¸Šæ‰§è¡Œã€‚

```json
{
  "steps": [
    {
      "type": "action",
      "action": "execute_command",
      "inputs": {
        "command": "find /host/data/daily -name '*.json' -mtime -7 -type f | sort",
        "environment_id": "{{environment_id}}"
      },
      "step_id": "list_weekly_files",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "execute_command",
      "inputs": {
        "command": "jq -s '{\n  arxiv:      (map(.arxiv      // []) | add // []),\n  github:     (map(.github     // []) | add // []),\n  huggingface:(map(.huggingface// []) | add // []),\n  hackernews: (map(.hackernews // []) | add // [])\n}' /host/data/daily/*.json",
        "environment_id": "{{environment_id}}"
      },
      "step_id": "aggregate_weekly_data",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "generate_text_response",
      "inputs": {
        "prompt": "You are a top-tier AI research analyst with deep expertise in machine learning, NLP, computer vision, and emerging AI technologies.\n\nAnalyze the following week's AI data and provide:\n\n1. **Top 3 Overarching Trends**: Identify the most significant macro-trends (e.g., 'Emergence of Small Language Models for Edge AI', 'Multimodal AI Breakthroughs', 'Open Source vs Closed Models Debate'). For each trend, explain WHY it matters and what it signals about the future.\n\n2. **Top 5 Most Important Items**: Rank the 5 most significant individual items (papers, repositories, models, or news) with a Significance Score (1-10). Consider factors like:\n   - Technical innovation and novelty\n   - Potential real-world impact\n   - Community interest and adoption\n   - Paradigm-shifting implications\n\n3. **Sharp Analysis**: For each trend and top item, provide concise but insightful commentary. Avoid generic statements.\n\n**Weekly Data:**\n```json\n{{aggregate_weekly_data.output.stdout}}\n```\n\nOutput your analysis in clear, well-structured Markdown format with proper headings and bullet points."
      },
      "step_id": "analyze_trends_and_rank",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "generate_structured_data",
      "inputs": {
        "prompt": "Based on the following AI trend analysis, create a practical, actionable learning plan for an AI developer/researcher for the upcoming week.\n\nFor each major trend or important development:\n- Suggest ONE concrete action item (e.g., 'Read paper X and implement key algorithm', 'Explore repo Y and run examples', 'Study model Z architecture')\n- Provide specific resource links when available\n- Prioritize items by learning value and time investment\n\n**Trend Analysis:**\n{{analyze_trends_and_rank.output.response}}\n\nRespond with ONLY a valid JSON object in this exact format:\n```json\n{\n  \"priority_topics\": [\n    {\n      \"topic\": \"Topic name\",\n      \"reason\": \"Why this topic is important to learn now\",\n      \"difficulty\": \"beginner|intermediate|advanced\"\n    }\n  ],\n  \"action_items\": [\n    {\n      \"task\": \"Specific action to take\",\n      \"resource_link\": \"URL to paper/repo/model\",\n      \"estimated_time\": \"Time estimate (e.g., '2 hours', '1 day')\",\n      \"priority\": \"high|medium|low\"\n    }\n  ]\n}\n```"
      },
      "step_id": "generate_learning_plan",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "execute_command",
      "inputs": {
        "command": "date +'%Y-W%V' | tr -d '\\n'",
        "environment_id": "{{environment_id}}"
      },
      "step_id": "get_week_identifier",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "generate_text_response",
      "inputs": {
        "prompt": "Create a polished, professional weekly report combining the trend analysis and learning plan below. Use this structure:\n\n# \ud83d\ude80 AI Frontier Weekly Report\n## Week {{get_week_identifier.output.stdout}}\n\n### \ud83d\udcca Executive Summary\n[2-3 sentence overview of the week's most important developments]\n\n### \ud83d\udd25 Top Trends\n[Insert trend analysis here with proper formatting]\n\n### \u2b50 Spotlight: Top 5 Developments\n[Insert ranked items with scores and analysis]\n\n### \ud83d\udcda Learning Plan for Next Week\n#### Priority Topics\n[Format the priority_topics from JSON as a clear list]\n\n#### Action Items\n[Format the action_items from JSON as a checklist with links]\n\n### \ud83d\udcc8 Statistics\n- Total Papers Tracked: [count from data]\n- Total GitHub Repos: [count from data]\n- Total Models: [count from data]\n- Total HN Stories: [count from data]\n\n---\n*Generated on {{get_week_identifier.output.stdout}} | Powered by AI Workflow Platform*\n\n**Trend Analysis:**\n{{analyze_trends_and_rank.output.response}}\n\n**Learning Plan (JSON):**\n```json\n{{generate_learning_plan.output.response}}\n```\n\n**Raw Data Summary:**\n```json\n{{aggregate_weekly_data.output.stdout}}\n```"
      },
      "step_id": "generate_weekly_report",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "execute_command",
      "inputs": {
        "command": "echo '{{generate_learning_plan.output.response}}' | python3 -c 'import sys, json; data = json.load(sys.stdin); print(json.dumps(data, indent=2, ensure_ascii=False))'",
        "environment_id": "{{environment_id}}"
      },
      "step_id": "serialize_learning_plan",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "write_files",
      "inputs": {
        "files": [
          {
            "content": "{{generate_weekly_report.output.response}}",
            "filepath": "data/weekly/report_{{get_week_identifier.output.stdout}}.md"
          },
          {
            "content": "{{serialize_learning_plan.output.stdout}}",
            "filepath": "data/weekly/learning_plan_{{get_week_identifier.output.stdout}}.json"
          },
          {
            "content": "{{aggregate_weekly_data.output.stdout}}",
            "filepath": "data/weekly/aggregated_data_{{get_week_identifier.output.stdout}}.json"
          }
        ],
        "environment_id": "{{environment_id}}"
      },
      "step_id": "save_weekly_outputs",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "execute_command",
      "inputs": {
        "command": "sudo cp -r data /host/",
        "environment_id": "{{environment_id}}"
      },
      "step_id": "copy_to_host",
      "return_as_final_answer": true
    }
  ]
}
```

---

**3. L3 - æŒ‰éœ€æ·±åº¦ç ”ç©¶å·¥ä½œæµ (`deep_dive_plan.json`)**

*   **ç›®æ ‡**: æ¥æ”¶ä¸€ä¸ªä¸»é¢˜ä½œä¸ºè¾“å…¥ï¼Œå›´ç»•è¯¥ä¸»é¢˜æ”¶é›†èµ„æ–™ã€é˜…è¯»è®ºæ–‡ã€ç”Ÿæˆå­¦ä¹ ç¬”è®°ï¼Œç”šè‡³ç¼–å†™ç¤ºä¾‹ä»£ç ã€‚
*   **è§¦å‘æ–¹å¼**: æ‰‹åŠ¨APIè°ƒç”¨ï¼Œä¼ å…¥`task`å‚æ•°ï¼Œå¦‚ `"task": "Deep dive on 'Retrieval-Augmented Generation'"`ã€‚

```json
{
  "name": "AI Topic Deep Dive Research",
  "steps": [
    {
      "type": "action",
      "action": "execute_command",
      "inputs": {
        "command": "echo '{{task}}' | python3 -c \"import sys, re, json; task=sys.stdin.read().strip(); match=re.search(r'[\\'\\\"](.+?)[\\'\\\"]', task); topic=match.group(1) if match else task.replace('Deep dive on ', '').replace('deep dive on ', ''); print(json.dumps({'topic': topic, 'slug': re.sub(r'[^a-z0-9]+', '_', topic.lower()).strip('_')}, ensure_ascii=False), end='')\" | tr -d '\\n'",
        "environment_id": "{{environment_id}}"
      },
      "step_id": "validate_and_extract_topic",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "execute_command",
      "inputs": {
        "command": "mkdir -p data/study_guides data/deep_dive",
        "environment_id": "{{environment_id}}"
      },
      "step_id": "create_directories",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "execute_command",
      "inputs": {
        "command": "TOPIC=$(echo '{{validate_and_extract_topic.output.stdout}}' | python3 -c 'import sys, json; print(json.loads(sys.stdin.read())[\"topic\"], end=\"\")'); bash sandbox_service/app/tools/search_arxiv_topic.sh \"$TOPIC\" 10",
        "environment_id": "{{environment_id}}"
      },
      "step_id": "search_arxiv_papers",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "execute_command",
      "inputs": {
        "command": "TOPIC=$(echo '{{validate_and_extract_topic.output.stdout}}' | python3 -c 'import sys, json; print(json.loads(sys.stdin.read())[\"topic\"], end=\"\")'); bash sandbox_service/app/tools/search_github_repos.sh \"$TOPIC\" 5",
        "environment_id": "{{environment_id}}"
      },
      "step_id": "search_github_repos",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "generate_text_response",
      "inputs": {
        "prompt": "You are an expert AI researcher. Analyze the following research papers on the topic from the research data below.\n\n**Topic Extraction Output:**\n{{validate_and_extract_topic.output.stdout}}\n\n**Papers Data:**\n```json\n{{search_arxiv_papers.output.stdout}}\n```\n\nFirst, extract the topic name from the Topic Extraction Output JSON, then provide:\n1. **Core Concepts Summary** (3-5 key ideas that define this topic)\n2. **Top 3 Must-Read Papers** (with title, why it's important, and key takeaway)\n3. **Technical Challenges** (what are the hard problems in this area)\n4. **Current State-of-the-Art** (what approaches are winning)\n\nFormat in clear Markdown with sections."
      },
      "step_id": "analyze_papers_with_ai",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "generate_text_response",
      "inputs": {
        "prompt": "You are a senior AI engineer. The topic extraction output is:\n\n{{validate_and_extract_topic.output.stdout}}\n\nAnalyze these GitHub repositories:\n\n```json\n{{search_github_repos.output.stdout}}\n```\n\nFirst extract the topic from the JSON above, then provide:\n1. **Ecosystem Overview** (what kinds of tools/libraries exist)\n2. **Top 2 Recommended Repos** (which ones to explore first and why)\n3. **Implementation Patterns** (common approaches you see in the codebases)\n\nBe concise and practical."
      },
      "step_id": "analyze_github_ecosystem",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "generate_structured_data",
      "inputs": {
        "prompt": "Based on the research above, create a structured learning roadmap.\n\n**Topic Info:**\n{{validate_and_extract_topic.output.stdout}}\n\n**Paper Analysis:**\n{{analyze_papers_with_ai.output.response}}\n\n**GitHub Ecosystem:**\n{{analyze_github_ecosystem.output.response}}\n\nFirst extract the topic name from the Topic Info JSON, then respond with ONLY valid JSON in this format:\n```json\n{\n  \"topic\": \"extracted topic name\",\n  \"prerequisites\": [\"concept1\", \"concept2\"],\n  \"learning_path\": [\n    {\n      \"phase\": \"1. Foundations\",\n      \"goals\": [\"goal1\", \"goal2\"],\n      \"resources\": [{\"type\": \"paper|repo|tutorial\", \"title\": \"...\", \"url\": \"...\"}],\n      \"estimated_time\": \"X hours/days\"\n    }\n  ],\n  \"hands_on_projects\": [\n    {\"title\": \"project name\", \"description\": \"what to build\", \"difficulty\": \"beginner|intermediate|advanced\"}\n  ]\n}\n```"
      },
      "step_id": "generate_learning_roadmap",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "generate_text_response",
      "inputs": {
        "prompt": "You are a Python expert. Based on the following research, write a complete, runnable Python code example that demonstrates the CORE concept.\n\n**Topic Info:**\n{{validate_and_extract_topic.output.stdout}}\n\nRequirements:\n- Include all necessary imports\n- Use only common libraries (transformers, torch, numpy, etc.)\n- Add detailed comments explaining each step\n- Keep it under 100 lines but fully functional\n- Include a simple usage example at the end\n\n**Research Context:**\n{{analyze_papers_with_ai.output.response}}\n\n**Available Libraries (from GitHub):**\n{{analyze_github_ecosystem.output.response}}\n\nFirst extract the topic from the Topic Info JSON, then provide ONLY the Python code, no explanations before or after."
      },
      "step_id": "generate_starter_code",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "execute_command",
      "inputs": {
        "command": "echo '{{validate_and_extract_topic.output.stdout}}' | python3 -c 'import sys, json; data=json.loads(sys.stdin.read()); print(data[\"topic\"], end=\"\")' | tr -d '\\n'",
        "environment_id": "{{environment_id}}"
      },
      "step_id": "extract_topic_name",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "execute_command",
      "inputs": {
        "command": "echo '{{validate_and_extract_topic.output.stdout}}' | python3 -c 'import sys, json; data=json.loads(sys.stdin.read()); print(data[\"slug\"], end=\"\")' | tr -d '\\n'",
        "environment_id": "{{environment_id}}"
      },
      "step_id": "extract_slug",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "generate_text_response",
      "inputs": {
        "prompt": "Compile everything into a polished study guide.\n\n**Topic:** {{extract_topic_name.output.stdout}}\n\nUse this structure:\n\n```markdown\n# ğŸ“š Deep Dive Study Guide: {{extract_topic_name.output.stdout}}\n\n> Generated on $(date +%Y-%m-%d) | Research Depth: Comprehensive\n\n## ğŸ¯ What You'll Learn\n[2-3 sentence overview of what this topic is and why it matters]\n\n## ğŸ§  Core Concepts\n[Insert paper analysis core concepts section]\n\n## ğŸ“– Essential Reading\n[Insert top papers from analysis]\n\n## ğŸ’» GitHub Ecosystem\n[Insert GitHub analysis]\n\n## ğŸ—ºï¸ Learning Roadmap\n[Format the JSON roadmap as readable sections with checkboxes]\n\n## ğŸ”¬ Hands-On: Starter Code\n[Insert the generated Python code in a code block]\n\n### How to Run\n```bash\npip install [required libraries]\npython starter_code.py\n```\n\n## ğŸš€ Next Steps\n- [ ] Read the top 3 papers\n- [ ] Clone and explore recommended repos\n- [ ] Modify the starter code for your use case\n- [ ] Build one of the hands-on projects\n\n## ğŸ“š Additional Resources\n- Official documentation links\n- Community forums\n- Related topics to explore next\n\n---\n*This guide was generated by AI Workflow Platform's Deep Dive system*\n```\n\n**Data to integrate:**\n\nPaper Analysis:\n{{analyze_papers_with_ai.output.response}}\n\nGitHub Analysis:\n{{analyze_github_ecosystem.output.response}}\n\nLearning Roadmap JSON:\n```json\n{{generate_learning_roadmap.output.response}}\n```\n\nStarter Code:\n```python\n{{generate_starter_code.output.response}}\n```\n\nProvide the complete, formatted study guide."
      },
      "step_id": "create_comprehensive_study_guide",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "write_files",
      "inputs": {
        "files": [
          {
            "content": "{{create_comprehensive_study_guide.output.response}}",
            "filepath": "data/study_guides/{{extract_slug.output.stdout}}.md"
          }
        ],
        "environment_id": "{{environment_id}}"
      },
      "step_id": "save_study_guide",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "write_files",
      "inputs": {
        "files": [
          {
            "content": "{{generate_starter_code.output.response}}",
            "filepath": "data/study_guides/{{extract_slug.output.stdout}}_code.py"
          }
        ],
        "environment_id": "{{environment_id}}"
      },
      "step_id": "save_starter_code",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "generate_text_response",
      "inputs": {
        "prompt": "Convert the following data to properly formatted JSON string. If it's already JSON, return it as-is. If it's a Python dict/object representation, convert it to valid JSON.\n\nData:\n{{generate_learning_roadmap.output.response}}\n\nReturn ONLY the JSON string, no explanations."
      },
      "step_id": "format_roadmap_json",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "write_files",
      "inputs": {
        "files": [
          {
            "content": "{{format_roadmap_json.output.response}}",
            "filepath": "data/deep_dive/{{extract_slug.output.stdout}}_roadmap.json"
          }
        ],
        "environment_id": "{{environment_id}}"
      },
      "step_id": "save_roadmap_json",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "write_files",
      "inputs": {
        "files": [
          {
            "content": "{{search_arxiv_papers.output.stdout}}",
            "filepath": "data/deep_dive/{{extract_slug.output.stdout}}_papers.json"
          }
        ],
        "environment_id": "{{environment_id}}"
      },
      "step_id": "save_papers_json",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "write_files",
      "inputs": {
        "files": [
          {
            "content": "{{search_github_repos.output.stdout}}",
            "filepath": "data/deep_dive/{{extract_slug.output.stdout}}_repos.json"
          }
        ],
        "environment_id": "{{environment_id}}"
      },
      "step_id": "save_repos_json",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "execute_command",
      "inputs": {
        "command": "sudo cp -r data /host/",
        "environment_id": "{{environment_id}}"
      },
      "step_id": "copy_to_host",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "execute_command",
      "inputs": {
        "command": "echo '{{search_arxiv_papers.output.stdout}}' | python3 -c 'import sys, json; data=json.loads(sys.stdin.read()); print(len(data.get(\"papers\", [])), end=\"\")' | tr -d '\\n'",
        "environment_id": "{{environment_id}}"
      },
      "step_id": "count_papers",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "execute_command",
      "inputs": {
        "command": "echo '{{search_github_repos.output.stdout}}' | python3 -c 'import sys, json; data=json.loads(sys.stdin.read()); print(len(data) if isinstance(data, list) else 0, end=\"\")' | tr -d '\\n'",
        "environment_id": "{{environment_id}}"
      },
      "step_id": "count_repos",
      "return_as_final_answer": false
    },
    {
      "type": "action",
      "action": "generate_text_response",
      "inputs": {
        "prompt": "Create a concise executive summary of the deep dive research completed.\n\n**Topic:** {{extract_topic_name.output.stdout}}\n\n**Outputs Generated:**\n- Study Guide: data/study_guides/{{extract_slug.output.stdout}}.md\n- Starter Code: data/study_guides/{{extract_slug.output.stdout}}_code.py\n- Learning Roadmap: data/deep_dive/{{extract_slug.output.stdout}}_roadmap.json\n\n**Research Stats:**\n- Papers Analyzed: {{count_papers.output.stdout}}\n- Repos Reviewed: {{count_repos.output.stdout}}\n\nProvide:\n1. ğŸ¯ **Key Insight** (one sentence: what's the most important thing to know)\n2. âš¡ **Quick Start** (what to do first to begin learning)\n3. ğŸ“Š **Complexity Rating** (beginner/intermediate/advanced and why)\n4. ğŸ”— **All Generated Files** (list with descriptions)\n\nKeep it under 200 words, actionable and encouraging."
      },
      "step_id": "generate_summary_report",
      "return_as_final_answer": true
    }
  ],
  "description": "Comprehensive research workflow for any AI topic: search papers, analyze repos, generate study guide with code examples"
}
```

### 4. å®æ–½è·¯çº¿å›¾ (Roadmap)

1.  **ç¬¬ä¸€å‘¨: å¥ å®šåŸºç¡€ (Foundation)**
    *   [ ] åœ¨`sandbox`ä¸­å¼€å‘å¹¶æµ‹è¯•è‡³å°‘3ä¸ªæ ¸å¿ƒé‡‡é›†å·¥å…· (`fetch_arxiv_daily.sh`, `fetch_github_trending_ai.sh`, `fetch_huggingface_trending.sh`)ã€‚
    *   [ ] æ‰‹åŠ¨æ‰§è¡Œ`daily_collector_plan.json`ï¼ŒéªŒè¯æ•°æ®èƒ½å¦è¢«æ­£ç¡®é‡‡é›†å’Œä¿å­˜ã€‚
    *   [ ] è®¾è®¡æ•°æ®å­˜å‚¨ç›®å½•ç»“æ„ (`/data/raw`, `/reports`, `/study_guides`)ã€‚

2.  **ç¬¬äºŒå‘¨: å®ç°è‡ªåŠ¨åŒ–ä¸åˆ†æ (Automation & Analysis)**
    *   [ ] é…ç½®å®šæ—¶ä»»åŠ¡ï¼Œå®ç°`daily_collector_plan`çš„æ¯æ—¥è‡ªåŠ¨æ‰§è¡Œã€‚
    *   [ ] å®Œå–„`weekly_analyzer_plan.json`ï¼Œå¹¶æ‰‹åŠ¨æ‰§è¡Œï¼Œè°ƒè¯•AIåˆ†æçš„Promptï¼Œç›´åˆ°äº§å‡ºæ»¡æ„çš„å‘¨æŠ¥ã€‚
    *   [ ] å°†`weekly_analyzer_plan`ä¹ŸåŠ å…¥å®šæ—¶ä»»åŠ¡ã€‚

3.  **ç¬¬ä¸‰å‘¨: é‡Šæ”¾æ·±åº¦å­¦ä¹ èƒ½åŠ› (Deep Dive & Integration)**
    *   [ ] å®Œå–„`deep_dive_plan.json`ï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®ç”¨æˆ·è¾“å…¥çš„ä¸»é¢˜ï¼Œå®Œæˆç ”ç©¶å’Œç¬”è®°ç”Ÿæˆã€‚
    *   [ ] ï¼ˆå¯é€‰ï¼‰é›†æˆé€šçŸ¥åŠŸèƒ½ï¼Œä¾‹å¦‚åœ¨å‘¨æŠ¥ç”Ÿæˆåï¼Œé€šè¿‡`execute_command`è°ƒç”¨`curl`å°†æŠ¥å‘Šå†…å®¹æ¨é€åˆ°Webhookï¼ˆä¼ä¸šå¾®ä¿¡ã€Slackã€Discordç­‰ï¼‰ã€‚

4.  **ç¬¬å››å‘¨åŠä»¥å: è¿­ä»£ä¸è¿›åŒ– (Evolution)**
    *   [ ] **åé¦ˆå¾ªç¯**: å¢åŠ ä¸€ä¸ªæœºåˆ¶ï¼Œè®©ä½ èƒ½å¯¹å‘¨æŠ¥ä¸­çš„æ¨èå†…å®¹è¿›è¡Œè¯„åˆ†ï¼Œå¹¶å°†è¯„åˆ†åé¦ˆç»™ä¸‹ä¸€å‘¨çš„åˆ†ææ¨¡å‹ï¼Œä»¥å®ç°ä¸ªæ€§åŒ–æ¨èã€‚
    *   **çŸ¥è¯†å›¾è°±**: è¿è¡Œä¸€ä¸ªé¢å¤–çš„AIæ­¥éª¤ï¼Œä»æ¯å‘¨çš„å®ä½“ï¼ˆè®ºæ–‡ã€é¡¹ç›®ã€æŠ€æœ¯ï¼‰ä¸­æå–å…³ç³»ï¼Œå¹¶æ„å»ºä¸€ä¸ªç®€å•çš„çŸ¥è¯†å›¾è°±ã€‚
    *   **è‡ªåŠ¨å®éªŒ**: æ¢ç´¢è®©L3å·¥ä½œæµä¸ä»…ç”Ÿæˆä»£ç ï¼Œè¿˜èƒ½åœ¨`sandbox`ç¯å¢ƒä¸­è‡ªåŠ¨æ‰§è¡Œä»£ç ï¼Œå¹¶å°†ç»“æœé™„åŠ åˆ°å­¦ä¹ ç¬”è®°ä¸­ã€‚

### 5. é¢„æœŸæˆæœ

é€šè¿‡å®æ–½æ­¤æ–¹æ¡ˆï¼Œä½ çš„AIå·¥ä½œæµå¹³å°å°†è½¬å˜ä¸ºä¸€ä¸ªå¼ºå¤§çš„ã€è‡ªä¸»çš„AIç ”ç©¶ç³»ç»Ÿï¼Œä¸ºä½ å¸¦æ¥ï¼š

*   âœ… **æ¯æ—¥10åˆ†é’Ÿ**ï¼Œå³å¯æŒæ¡AIé¢†åŸŸçš„å…³é”®åŠ¨æ€ï¼Œå‘Šåˆ«ä¿¡æ¯ç„¦è™‘ã€‚
*   âœ… **æ¯å‘¨1å°æ—¶**ï¼Œè·å¾—ä¸€ä»½åŒ…å«æ·±åˆ»æ´å¯Ÿå’Œå®šåˆ¶åŒ–å­¦ä¹ è·¯å¾„çš„æ·±åº¦æŠ¥å‘Šã€‚
*   âœ… **æŒ‰éœ€å¯åŠ¨**ï¼Œå³å¯åœ¨æ•°åˆ†é’Ÿå†…è·å¾—ä»»ä½•ä¸€ä¸ªAIæ–°ä¸»é¢˜çš„å®Œæ•´å­¦ä¹ å…¥é—¨åŒ…ã€‚
*   âœ… **çœŸæ­£å®ç°**â€œè®©AIä¸ºä½ å·¥ä½œâ€ï¼Œå°†ä½ çš„æ—¶é—´å’Œç²¾åŠ›èšç„¦äºæœ€é«˜ä»·å€¼çš„åˆ›é€ æ€§æ´»åŠ¨ä¸Šã€‚
```