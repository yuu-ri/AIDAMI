関数 f(x) = 1 + 2x + 3x² を学習させるシンプルな3層ニューラルネットワークの設計です。

1. 入力層  
   A[0] = X

2. 隠れ層  
   Z[1] = W[1] ⋅ A[0] + b[1]  
   A[1] = sigmoid(Z[1])

3. 出力層  
   Z[2] = W[2] ⋅ A[1] + b[2]  
   A[2] = Z[2]

4. **順伝播の流れ図**

```
X（入力）
  │
  ↓  乘以 W[1] + b[1]
Z[1]（隠れ層の線形出力）
  ↓  シグモイド活性化関数を通す
A[1]（隠れ層の活性化後出力）
  │
  ↓  乘以 W[2] + b[2]
Z[2]（出力層の線形出力）
  ↓  （出力層は活性化なし、直接出力）
Y_hat（最終予測値）
```

5. Y と Y_hat の定義  
   - **Y**: 正解値。入力 X に対し Y = 1 + 2X + 3X²  
   - **Y_hat**: ネットワークの予測値。Y_hat = A[2] = Z[2]

6. 損失関数  
   L = (1/N) × Σ (Y_hat - Y)²

7. 誤差逆伝播のための勾配式（詳細な導出過程を保持）

   連鎖律を使って1つずつ丁寧に導出します。まず1サンプルの損失 l = (Y_hat - Y)² で計算し、最後にN個平均します。

   **(a) 出力層の重み W[2] に対する勾配**

   ∂l / ∂W[2] = ∂l / ∂Y_hat × ∂Y_hat / ∂Z[2] × ∂Z[2] / ∂W[2]

   - ∂l / ∂Y_hat = 2 (Y_hat - Y)  
   - ∂Y_hat / ∂Z[2] = 1（線形活性化）  
   - ∂Z[2] / ∂W[2] = A[1]（出力層への入力）

   よって1サンプル：  
   ∂l / ∂W[2] = 2 (Y_hat - Y) * A[1]

   Nサンプル平均：  
   dL/dW[2] = (1/N) * sum(2 * (Y_hat - Y)) * A[1]

   **(b) 隠れ層の重み W[1] に対する勾配**

   さらに連鎖を続けます：

   ∂l / ∂W[1] = ∂l / ∂Y_hat × ∂Y_hat / ∂Z[2] × ∂Z[2] / ∂A[1] × ∂A[1] / ∂Z[1] × ∂Z[1] / ∂W[1]

   - 最初の2項：2 (Y_hat - Y)  
   - ∂Z[2] / ∂A[1] = W[2]  
   - ∂A[1] / ∂Z[1] = sigmoid'(Z[1]) = A[1] ⋅ (1 - A[1])（sigmoid_derivative(A[1])）  
   - ∂Z[1] / ∂W[1] = A[0]（＝X）

   よって1サンプル：  
   ∂l / ∂W[1] = 2 (Y_hat - Y) * W[2] * sigmoid_derivative(A[1]) * A[0]

   Nサンプル平均：  
   dL/dW[1] = (1/N) * sum(2 * (Y_hat - Y) * W[2] * sigmoid_derivative(A[1])) * A[0]

   バイアスの勾配は同様に計算（最後の入力項を1に置き換え）。

